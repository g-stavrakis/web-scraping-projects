{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c13688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:52.192120Z",
     "start_time": "2021-11-03T20:12:52.179124Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04028c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:52.207119Z",
     "start_time": "2021-11-03T20:12:52.195116Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b4d48",
   "metadata": {},
   "source": [
    "#  Briefly describe all the issues you can find in data gathering and what they are caused by."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f18a07",
   "metadata": {},
   "source": [
    "## Code from the DTVC_Week 2_Exercise notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77407aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:52.316151Z",
     "start_time": "2021-11-03T20:12:52.209116Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_aldi_jobs(starting_page = 'head-office'):\n",
    "    url = \"https://www.aldirecruitment.co.uk/\" + starting_page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        new_link = link.get('href')\n",
    "        if new_link != None and new_link.startswith('/head-office/'):\n",
    "            new_link = new_link.replace('/head-office','')\n",
    "            if new_link != '/':\n",
    "                links.append(new_link)\n",
    "    \n",
    "    department = []\n",
    "    titles = []\n",
    "    ubs = []\n",
    "    lbs = []\n",
    "    hours = []\n",
    "    for link in links:\n",
    "        category_url = url + link\n",
    "        page = requests.get(category_url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        postings = soup.findAll(\"div\", class_=\"c-career--dropdown\")\n",
    "        for posting in postings:\n",
    "            ## Also grabbing the department information\n",
    "            dep_name = link.replace('-',' ').replace('/','')\n",
    "            department.append(dep_name)\n",
    "            titles.append(posting.find(\"div\", class_=\"c-career--dropdown__content\").find('h2').text)\n",
    "            details = posting.findAll(\"div\", class_=\"c-job-details__content\")\n",
    "            for detail in details:\n",
    "                detail_title = detail.find('span', class_=\"c-job-details__title\").text\n",
    "                detail_text = detail.find('div', class_=\"c-job-details__text\").text\n",
    "                if detail_title == 'Salary':\n",
    "                    temp = detail_text.replace(',','')\n",
    "                    temp = temp.replace('-','')\n",
    "                    temp = temp.split()\n",
    "                    salary_numbers = [float(s[1:]) for s in temp if s.startswith('Â£')]\n",
    "                    ## Salary may not be specified\n",
    "                    if len(salary_numbers) > 0:\n",
    "                        ## Salaries are sometimes specified as per week instead of per year\n",
    "                        if 'per' in temp and 'week' in temp:\n",
    "                            salary_numbers = [salary*52 for salary in salary_numbers]\n",
    "                        ubs.append(max(salary_numbers))\n",
    "                        lbs.append(min(salary_numbers))\n",
    "                    else:\n",
    "                        ubs.append(None)\n",
    "                        lbs.append(None)\n",
    "                ## Some postings say \"Benefits\" instead of \"Hours and benefits\", and sometimes the spelling is capitalized differently\n",
    "                elif detail_title.lower() == 'hours and benefits' or detail_title.lower() == 'benefits':\n",
    "                    ## Some postings do not specify a number of hours per week\n",
    "                    work_time = None\n",
    "                    for s in detail_text.split():\n",
    "                        if '-hour' in s:\n",
    "                            work_time = s\n",
    "                            ## Some postings write, e.g., 40-hour per week, some 40-hours per week\n",
    "                            if '-hours' in s:\n",
    "                                work_time = int(work_time.replace('-hours',''))\n",
    "                            else:\n",
    "                                work_time = int(work_time.replace('-hour',''))\n",
    "                    hours.append(work_time)\n",
    "                        \n",
    "    job_data = pd.DataFrame(\n",
    "        {'Department': department,\n",
    "         'Job title': titles,\n",
    "         'Salary lower': lbs,\n",
    "         'Salary upper': ubs,\n",
    "         'Weekly hours': hours\n",
    "        })\n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9ba5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:55.691774Z",
     "start_time": "2021-11-03T20:12:52.318115Z"
    }
   },
   "outputs": [],
   "source": [
    "jobs_data = scrape_aldi_jobs()\n",
    "jobs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b197ee",
   "metadata": {},
   "source": [
    "## Errors appeared in the data gathering function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b00e37",
   "metadata": {},
   "source": [
    "- The values of lower and upper salaries are not appropriately gathered (ex. 4.00 and 46435.00)\n",
    "    - Reason: We didn't include a condition for salaries displayed in hour rate (ex. Customer Service Advisor, Quality Control Assistant)\n",
    "    - Reason: We didn't include a condition for premium as an additional salary's element in the min function for the lower salary (ex. Quality Control Deputy)\n",
    "    - Reason: Data Scientist posting has a typo in the lower salary. It has an extra space between comma and hundreds.\n",
    "- The values of lower and upper salaries have NaN Values\n",
    "    - Reason: The posting didn't specify a salary\n",
    "- We have some NaN values in the weekly hours\n",
    "    - Reason: The posting didn't specify weekly hours\n",
    "    - Reason: The posting specify weekly hours without hyphen (ex. 40-hours per week, 40 hours per week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9448df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:55.707818Z",
     "start_time": "2021-11-03T20:12:55.693774Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.min(jobs_data['Weekly hours']), np.max(jobs_data['Weekly hours']), jobs_data['Weekly hours'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1d3e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:55.723832Z",
     "start_time": "2021-11-03T20:12:55.709778Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.min(jobs_data['Salary lower']), np.max(jobs_data['Salary lower']), jobs_data['Salary lower'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6ba14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:55.739778Z",
     "start_time": "2021-11-03T20:12:55.725776Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.min(jobs_data['Salary upper']), np.max(jobs_data['Salary upper']), jobs_data['Salary upper'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59fddeb",
   "metadata": {},
   "source": [
    "# Rewrite the scraping function to overcome the issues you encountered above, collect additional information that could be sensible to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6758ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:55.803103Z",
     "start_time": "2021-11-03T20:12:55.746030Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_scrape_aldi_jobs(starting_page = 'head-office'):\n",
    "    url = \"https://www.aldirecruitment.co.uk/\" + starting_page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        new_link = link.get('href')\n",
    "        if new_link != None and new_link.startswith('/head-office/'):\n",
    "            new_link = new_link.replace('/head-office','')\n",
    "            if new_link != '/':\n",
    "                links.append(new_link)\n",
    "    \n",
    "    department = []\n",
    "    titles = []\n",
    "    ubs = []\n",
    "    lbs = []\n",
    "    hours = []\n",
    "    \n",
    "    years = []\n",
    "    holiday = []\n",
    "    medical = []   \n",
    "    mothers = []\n",
    "    fathers = []\n",
    "    \n",
    "    for link in links:\n",
    "        category_url = url + link\n",
    "        page = requests.get(category_url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        postings = soup.findAll(\"div\", class_=\"c-career--dropdown\")\n",
    "        for posting in postings:\n",
    "            ## Also grabbing the department information\n",
    "            dep_name = link.replace('-',' ').replace('/','')\n",
    "            department.append(dep_name)\n",
    "            titles.append(posting.find(\"div\", class_=\"c-career--dropdown__content\").find('h2').text)\n",
    "            details = posting.findAll(\"div\", class_=\"c-job-details__content\")\n",
    "            for detail in details:\n",
    "                detail_title = detail.find('span', class_=\"c-job-details__title\").text\n",
    "                detail_text = detail.find('div', class_=\"c-job-details__text\").text\n",
    "    \n",
    "                if detail_title == 'Salary':\n",
    "                    temp = detail_text.replace(',','')\n",
    "                    temp = temp.replace('-','')\n",
    "                    temp = temp.split()\n",
    "                    \n",
    "                    ## Some postings state in each year the employee will reach his/her max salary            \n",
    "                    yrs = None\n",
    "                    if 'year' in temp:    \n",
    "                        Index = temp.index('year')\n",
    "                        temp_year = temp[Index+1]\n",
    "                        if 'after' in temp:\n",
    "                            Index = temp.index('after')\n",
    "                            temp_year = temp[Index+1]\n",
    "                        yrs =temp_year.replace('.','')\n",
    "                        if ').' in temp_year:\n",
    "                            yrs =temp_year.replace(').','')\n",
    "                    if yrs == None:\n",
    "                        years.append(yrs)\n",
    "                    else:\n",
    "                        years.append(int(yrs))\n",
    "                                               \n",
    "                    salary_numbers = [float(s[1:]) for s in temp if s.startswith('Â£')]\n",
    "                    ## Salary may not be specified\n",
    "                    if len(salary_numbers) > 0:\n",
    "                        ## Salaries are sometimes specified as per week instead of per year\n",
    "                        if 'per' in temp and 'week' in temp:\n",
    "                            salary_numbers = [salary*52 for salary in salary_numbers]\n",
    "                        ## Salaries are sometimes specified as per hours instead of per year\n",
    "                        elif 'per' in temp and 'hour' in temp: \n",
    "                            salary_numbers = [(salary*40)*52 for salary in salary_numbers]    \n",
    "                        ## Salaries are sometimes specified a premium which we want to exclude from lower salary \n",
    "                        if 'premium' in temp:\n",
    "                            salary_numbers.sort()\n",
    "                            salary_numbers = salary_numbers[1:]\n",
    "                        ubs.append(max(salary_numbers))\n",
    "                        lbs.append(min(salary_numbers))\n",
    "                    else:\n",
    "                        ubs.append(None)\n",
    "                        lbs.append(None)\n",
    "                ## Some postings say \"Benefits\" instead of \"Hours and benefits\", and sometimes the spelling is capitalized differently\n",
    "                elif detail_title.lower() == 'hours and benefits' or detail_title.lower() == 'benefits':\n",
    "\n",
    "                   ## Some postings provide medical insurance after some years or months \n",
    "                    med = 0\n",
    "                    if 'medical' in detail_text.split():\n",
    "                        med = 1         \n",
    "                    medical.append(med)         \n",
    "                    \n",
    "                    ## Some postings do not specify a number of hours per week\n",
    "                    work_time = None\n",
    "                    for s in detail_text.split():\n",
    "                        if '-hour' in s:\n",
    "                            work_time = s\n",
    "                            ## Some postings write, e.g., 40-hour per week, some 40-hours per week\n",
    "                            if '-hours' in s:\n",
    "                                work_time = int(work_time.replace('-hours',''))\n",
    "                            else:\n",
    "                                work_time = int(work_time.replace('-hour',''))\n",
    "                    ## Some postings write, e.g., 40hour per week, some 40-hour per week \n",
    "                    if 'hours' in detail_text.split():        \n",
    "                            Index = detail_text.split().index('hours')\n",
    "                            work_time = int(detail_text.split()[Index-1])\n",
    "                    hours.append(work_time)\n",
    "                \n",
    "                    ## Some postings provide maternity leave\n",
    "                    mother = 0\n",
    "                    if ('maternity' in detail_text.split()) or ('maternity,' in detail_text.split()):\n",
    "                        mother = 1\n",
    "                    mothers.append(mother)    \n",
    "                    \n",
    "                    ## Some postings provide paternity pay\n",
    "                    father = 0\n",
    "                    if ('paternity' in detail_text.split()) or ('paternity,' in detail_text.split()):\n",
    "                        father = 1                 \n",
    "                    fathers.append(father)                \n",
    "                \n",
    "                ## Some postings say \"Holiday Allowance\" instead of \"Holiday\", and sometimes the spelling is capitalized differently\n",
    "                ## One posting have a typo of \"Holiday allownance\"\n",
    "                elif detail_title.lower() == 'holiday' or detail_title.lower() == 'holiday allowance' or detail_title.lower() == 'holiday allownance':                                                            \n",
    "                    off_time = None\n",
    "                    ## Some postings specify a number of weeks for holiday days\n",
    "                    if 'weeksâ' in detail_text.split():        \n",
    "                        Index = detail_text.split().index('weeksâ')\n",
    "                        off_time = int(detail_text.split()[Index-1])\n",
    "                    ## Some postings specify a number of days for holiday days\n",
    "                    elif 'days' in detail_text.split():\n",
    "                        Index = detail_text.split().index('days')\n",
    "                        off_time = (int(detail_text.split()[Index-1]))/7\n",
    "                    elif 'daysâ' in detail_text.split():\n",
    "                        Index = detail_text.split().index('daysâ')\n",
    "                        off_time = (int(detail_text.split()[Index-1]))/7\n",
    "                    holiday.append(off_time)\n",
    "                    \n",
    "    job_data = pd.DataFrame(\n",
    "        {'Department': department,\n",
    "         'Job title': titles,\n",
    "         'Salary lower': lbs,\n",
    "         'Salary upper': ubs,\n",
    "         'Years to upper salary': years,\n",
    "         'Weekly hours': hours,\n",
    "         'Holiday weeks': holiday,\n",
    "         'Maternity leave': mothers,\n",
    "         'Paternity pay': fathers,\n",
    "         'Medical insurance': medical         \n",
    "        })\n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4c9a9",
   "metadata": {},
   "source": [
    "The additional information that we collected are the following:\n",
    "- Holiday Allowance: We collected this data because we expect this to effect the salary given in each position. Also, this information is good to know in order to understand what our competitors give as common perk to potential candidates for each position. The first issue we encounter during the collection of this data was mostly due to differences in the title of the section in job description ('Holiday Allowance'/'Holiday'/'holiday allownance'(typo)). The second issue was that some postings specify the Holiday Allowance is terms of weeks and others in terms on days so we needed to convert that properly.\n",
    "- Years to upper salary: We collected this data because we expect an effect of the amount of Max Salary someone can reach based on the years needed to reach it. The first issue we encounter here is that some postings have different sentence structure (ex. Quality Control Assistant & Quality Control Deputy) so we need to include this differences in the if statements. The second issue was that the numbers came with the following punctuation with them so we need to remove them in order to convert them to integers.\n",
    "- Medical insurance: We collected this data in order to identify if it's common for our competitors to provide medical insurance as an extra benefit. We didn't encounter any issues in the collection of these data.\n",
    "- Maternity leave & Paternity Pay: We collected this data in order to identify if it is common to provide extra perks regarding parenting in our competitors job descriptions. We didn't encounter any issues in the collection of these data.\n",
    "\n",
    "The Medical insurance along with the Maternity leave & Paternity Pay was collected not to test how they affect the salaries but in order to have a more complete picture of the minimum requirement appear in our competitors as extra benefits.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d536c96",
   "metadata": {},
   "source": [
    "# Run your revised scraping function to collect information on postings and save the information as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ac7ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.198965Z",
     "start_time": "2021-11-03T20:12:55.810030Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs = new_scrape_aldi_jobs()\n",
    "new_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa161a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.213942Z",
     "start_time": "2021-11-03T20:12:59.200957Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.to_csv('Aldi_postings_revised.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02cd58f",
   "metadata": {},
   "source": [
    "# Perform data pre-processing steps (cleansing, scaling, normalization, imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23641fc2",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afc3dc",
   "metadata": {},
   "source": [
    "1. Checking for invalid values. Because the columns Salary lower and Salary upper contain only numeric values we will check for invalid ones by display them in a histogram and see if the plots are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c0deb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.229957Z",
     "start_time": "2021-11-03T20:12:59.216927Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358ca7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.620835Z",
     "start_time": "2021-11-03T20:12:59.231918Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax0 = fig.add_subplot(1,2,1)\n",
    "ax0.hist('Salary lower',  data= new_jobs, edgecolor='white')\n",
    "ax0.set_title('Salary lower')\n",
    "ax1 = fig.add_subplot(1,2,2)\n",
    "ax1.hist('Salary upper', data= new_jobs, edgecolor='white')\n",
    "ax1.set_title('Salary upper')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10673953",
   "metadata": {},
   "source": [
    "We detected that the majority of the minimum salary values are concentrated between 20000 and 60000. However, We noted that some lower salaries are at the range of 80000, so we need to examine if they are so high by mistake or have equally large upper salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d6177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.650882Z",
     "start_time": "2021-11-03T20:12:59.622836Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Salary lower']>=75000,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96f5a9",
   "metadata": {},
   "source": [
    "Result: So the lower salaries are correctly gathered and the reason that they are separated from the others is that they are paired with positions of Senior Managers (higher paid positions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf52cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T21:06:11.246602Z",
     "start_time": "2021-10-31T21:06:11.043609Z"
    }
   },
   "source": [
    "1.Checking for invalid values also at columns Maternity leave, Paternity pay and Medical insurance which contain categorical values. We will check for invalid values by checking that their unique values are either 0 or 1. In Addition columns like Years to upper salary, Holiday weeks and weekly hours can have a limited range of values so we will check them as categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4257b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.666836Z",
     "start_time": "2021-11-03T20:12:59.652837Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Maternity leave: ' ,new_jobs['Maternity leave'].unique())\n",
    "print('Paternity pay: ', new_jobs['Paternity pay'].unique())\n",
    "print('Medical insurance: ', new_jobs['Medical insurance'].unique())\n",
    "print('Years to upper salary: ',new_jobs['Years to upper salary'].unique())\n",
    "print('Holiday weeks: ', new_jobs['Holiday weeks'].unique())\n",
    "print('Weekly hours: ', new_jobs['Weekly hours'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f2e86",
   "metadata": {},
   "source": [
    "In general we didn't spot any extreme value that can be identified as invalid. However, in 'Years to upper salary' column the value 1 sims kind of small in regard to the others. Moreover, in 'Weekly hours' column, the value of 20 sees kind of small and looks suspicious if the corresponding job is not part-time. We will examine the two values further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc429869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.713837Z",
     "start_time": "2021-11-03T20:12:59.668836Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Years to upper salary']== 1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168891b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.743837Z",
     "start_time": "2021-11-03T20:12:59.715835Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Weekly hours']== 20,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7340f48",
   "metadata": {},
   "source": [
    "2. Examine if there are duplicates in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d1392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.775872Z",
     "start_time": "2021-11-03T20:12:59.746834Z"
    }
   },
   "outputs": [],
   "source": [
    "double = new_jobs.duplicated()\n",
    "print(double.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaa49d",
   "metadata": {},
   "source": [
    "Results: There are no duplicates in the dataset because the double.any() function return False, which means there is none true value in the double variable. So we don't need to modify our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd15418",
   "metadata": {},
   "source": [
    "3. Checking for row records that have empty the salary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941fa02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.807841Z",
     "start_time": "2021-11-03T20:12:59.777842Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Salary upper'].isna(),: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bde3c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.837836Z",
     "start_time": "2021-11-03T20:12:59.809838Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Salary lower'].isna(),: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d3259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T23:19:09.355815Z",
     "start_time": "2021-11-01T23:19:09.332814Z"
    }
   },
   "source": [
    "We detect in the dataset that Dicterors' postings didn't specify the salary probably because the offers become personalized based on the candidate. So because they didn't specify the salary we will drop them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236271d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.869834Z",
     "start_time": "2021-11-03T20:12:59.841837Z"
    }
   },
   "outputs": [],
   "source": [
    "for Index,title in enumerate(new_jobs['Job title']):\n",
    "    if 'Director' in title:\n",
    "        new_jobs.drop(index=Index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe475bb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.900842Z",
     "start_time": "2021-11-03T20:12:59.874331Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Salary lower'].isna(),: ], new_jobs.loc[new_jobs['Salary upper'].isna(),: ] , len(new_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529342e",
   "metadata": {},
   "source": [
    "Results: We delete the rows which have postings for Director positions and we reduced our dataset from 99 rows to 95."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3202b2",
   "metadata": {},
   "source": [
    "4. Examine if any one of the columns have the same value for every row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a0e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.930871Z",
     "start_time": "2021-11-03T20:12:59.902835Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695eeff",
   "metadata": {},
   "source": [
    "Results: None of the columns contained the same value in each row (all the values are higher than 1), so we don't need to modify the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b7f44",
   "metadata": {},
   "source": [
    "## Data imputation for numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57c36c",
   "metadata": {},
   "source": [
    "In the data cleansing process we detect that the weekly hours and the Years to upper salary columns have some missing values due to the fact that there are no specified in the job description. In order to fill those missing values we will randomly select one value from the jobs in the same departments which are closer to the missing job's description.  \n",
    "The process is the following:\n",
    "- Find the rows with the missing weekly hours\n",
    "- Generate values based on different departments through random choice\n",
    "- Assign the appropriate values to the missing rows (We did this manually because there are not many missing values. If the number of get bigger we will use the impute library from sklearn)\n",
    "- Check if there are other missing values in this column using unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e84585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:12:59.976841Z",
     "start_time": "2021-11-03T20:12:59.938884Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[pd.isna(new_jobs['Weekly hours']),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73bb9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.007836Z",
     "start_time": "2021-11-03T20:12:59.981837Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(222)\n",
    "random = new_jobs.groupby('Department')['Weekly hours'].aggregate(np.random.choice)\n",
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936eb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.037835Z",
     "start_time": "2021-11-03T20:13:00.011863Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Job title'] == 'National Administration Analyst', 'Weekly hours'] = random['national administration']\n",
    "new_jobs.loc[new_jobs['Job title'] == 'eCommerce Analyst','Weekly hours'] = random['ecommerce']\n",
    "new_jobs.loc[new_jobs['Job title'] == 'eCommerce Associate Analyst','Weekly hours'] = random['ecommerce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3536c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.053834Z",
     "start_time": "2021-11-03T20:13:00.040849Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Weekly hours: ', new_jobs['Weekly hours'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931144f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.084832Z",
     "start_time": "2021-11-03T20:13:00.055836Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[pd.isna(new_jobs['Years to upper salary']),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9674fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.115836Z",
     "start_time": "2021-11-03T20:13:00.086836Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(111)\n",
    "random1 = new_jobs.groupby('Department')['Years to upper salary'].aggregate(np.random.choice)\n",
    "random1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967738a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.146833Z",
     "start_time": "2021-11-03T20:13:00.117840Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs.loc[new_jobs['Job title'] == 'National Administration Apprentice','Years to upper salary'] = random1['national administration']\n",
    "new_jobs.loc[new_jobs['Job title'] == 'National IT Apprentice','Years to upper salary'] = random1['it']\n",
    "new_jobs.loc[new_jobs['Job title'] == 'Quality Control Deputy','Years to upper salary'] = random1['quality control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f944ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.161835Z",
     "start_time": "2021-11-03T20:13:00.149835Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Weekly hours: ', new_jobs['Weekly hours'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462a3dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.207835Z",
     "start_time": "2021-11-03T20:13:00.164839Z"
    }
   },
   "outputs": [],
   "source": [
    "new_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc94741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T21:48:11.280488Z",
     "start_time": "2021-11-02T21:48:11.265334Z"
    }
   },
   "source": [
    "# Featuring engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762199b",
   "metadata": {},
   "source": [
    "Because we want to examine the possible effects of our predictors on the average salary we need to calculate a feature with the average from the upper and lower salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276fe9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.223835Z",
     "start_time": "2021-11-03T20:13:00.210840Z"
    }
   },
   "outputs": [],
   "source": [
    "average_salary = (new_jobs['Salary lower'] + new_jobs['Salary upper'])/2\n",
    "average_salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc78c1",
   "metadata": {},
   "source": [
    "Also we will include in my regression the job title and examine it's effect of the average salary. To do this we need to transform the job titles from strings to numerical values. More specifically we want to examine how much is the effect of the job hierarchy titles in the average salary. So we will interpret the job titles as categorical values based on the hierarchy titles as follows for lower to higher:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65ee80",
   "metadata": {},
   "source": [
    "- Apprentice\n",
    "- Assistant (and also Technician / Deputy / Advisor)\n",
    "- Analyst  (Technologist / Chef)\n",
    "- Specialist\n",
    "- Leader\n",
    "- Scientist\n",
    "- Manager\n",
    "- Senior Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229b5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.238835Z",
     "start_time": "2021-11-03T20:13:00.225837Z"
    }
   },
   "outputs": [],
   "source": [
    "title_num =[]\n",
    "for title in new_jobs['Job title']:\n",
    "    if 'Apprentice' in title:\n",
    "        title_num.append(1)\n",
    "    elif 'Assistant' in title:\n",
    "        title_num.append(2)\n",
    "    elif 'Technician' in title:\n",
    "        title_num.append(2)  \n",
    "    elif 'Deputy' in title:\n",
    "        title_num.append(2)\n",
    "    elif 'Advisor' in title:\n",
    "        title_num.append(2)\n",
    "    elif 'Analyst' in title:\n",
    "        title_num.append(3)\n",
    "    elif 'Chef' in title:\n",
    "        title_num.append(3)\n",
    "    elif 'Technologist' in title:\n",
    "        title_num.append(3)\n",
    "    elif 'Specialist' in title:\n",
    "        title_num.append(4)\n",
    "    elif 'Administrator' in title:\n",
    "        title_num.append(4)\n",
    "    elif 'Leader' in title:\n",
    "        title_num.append(5)\n",
    "    elif 'Co-ordinator' in title:\n",
    "        title_num.append(5)\n",
    "    elif 'Scientist' in title:\n",
    "        title_num.append(6)\n",
    "    elif 'Manager' in title:\n",
    "        title_num.append(7)\n",
    "    elif 'Senior Manager' in title:\n",
    "        title_num.append(8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23d28f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.254839Z",
     "start_time": "2021-11-03T20:13:00.240836Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'average_salary':average_salary,'Years to upper salary': new_jobs['Years to upper salary'],\n",
    "                     'Weekly hours': new_jobs['Weekly hours'],'Holiday weeks': new_jobs['Holiday weeks'],\n",
    "                    'Job_title_Num': title_num})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3148c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.285837Z",
     "start_time": "2021-11-03T20:13:00.257838Z"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d5554",
   "metadata": {},
   "source": [
    "# Formulate a hypothesis about the data and test this hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5add30c",
   "metadata": {},
   "source": [
    "Hypothesis: The annual average salary specified in one job posting can be a predicted based on the weekly working hours, the hierarchy level of the job, the number of years you need to progress into the upper salary and the holiday weeks specified in the job description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415531b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.301839Z",
     "start_time": "2021-11-03T20:13:00.287836Z"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adb339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.349870Z",
     "start_time": "2021-11-03T20:13:00.303836Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X =data[['Job_title_Num','Years to upper salary','Weekly hours','Holiday weeks']]\n",
    "Y = data[['average_salary']]\n",
    "X = sm.add_constant(X)\n",
    "lm = sm.OLS(Y,X).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1aad37",
   "metadata": {},
   "source": [
    "Statistical Interpretation of our results:\n",
    "\n",
    "Based on the results of our regression we can see that our predictors (Job_title_Num, Years to upper salary, Weekly hours, Holiday weeks ) explain the 70.5% (based on the Adjusted R-squared) of the variability of the average salaries in the job postings. In Addition, we can reject the null hypothesis (that there is no linear relationship between my predictors and me response) and we can say that at least one predictor have a linear relationship with the average salaries. More specifically, we have to predictors that are statistically significant which are the weekly working hours and the numeric representation of Job titles. The Years to upper salary and Holiday weeks aren't statistically significant. \n",
    "\n",
    "Note: The extremely low p-value of the numeric representation of Job titles is probably to my interpretation of the order of the hierarchy status based on the salary so we can't assess it properly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b63bca",
   "metadata": {},
   "source": [
    "Business Interpretation of our results:\n",
    "\n",
    "- We can see that all predictors have positive coefficient with the average salary, so when the value of any predictor rise will also rise the average salary.\n",
    "\n",
    "More specifically about the statistically significant predictors:\n",
    "\n",
    "- If the weekly hours rise by 1 base point - meaning rise by 10 hours - this will result to Â£876.96 rise in the annual average salary.\n",
    "- If the Job title rise by 1 base point - meaning rise by one hierarchy level (from Apprentice to Assistant)- this will result to Â£4843.57 rise in the annual average salary.\n",
    "\n",
    "More specifically about the not statistically significant predictors:\n",
    "\n",
    "- If the Years to upper salary rise by 1 base point - meaning rise by 1 year - this will result to Â£214.35 rise in the annual average salary.\n",
    "- If the Holiday weeks rise by 1 base point - meaning rise by 1 week - this will result to Â£3661.10 rise in the annual average salary which looks kind of weird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d293df",
   "metadata": {},
   "source": [
    "Because our first hypothesis contained some variables that didn't have a statistically significant result we will try a much simpler regression model with only the significant predictors (Weekly hours and Job title) and check if this has an impact of the R squared of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04497ae4",
   "metadata": {},
   "source": [
    "New Hypothesis: The annual average salary affected by the weekly working hours and the hierarchy level of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55567f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.381833Z",
     "start_time": "2021-11-03T20:13:00.351836Z"
    }
   },
   "outputs": [],
   "source": [
    "X1 =data[['Job_title_Num','Weekly hours']]\n",
    "Y = data[['average_salary']]\n",
    "X1 = sm.add_constant(X1)\n",
    "lm1 = sm.OLS(Y,X1).fit()\n",
    "print(lm1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77162f06",
   "metadata": {},
   "source": [
    "Statistical Interpretation of our results:\n",
    "\n",
    "Based on the results of our second regression we can see that even with the reduced number of predictors (Job_title_Num, Weekly hours) the model still explains the 70% (based on the Adjusted R-squared) of the variability of the average salaries in the job postings. In Addition, we can still reject the null hypothesis and we can say that at least one of the two predictor have a linear relationship with the average salaries. Lastly, both our two predictors now have extremely small p-value so are still both statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738f878",
   "metadata": {},
   "source": [
    "Business Interpretation of our results:\n",
    "\n",
    "- Still we can see that our two predictors have positive coefficient with the average salary, so when the value of any predictor rise will also rise the average salary.\n",
    "\n",
    "More specifically about the statistically significant predictors:\n",
    "\n",
    "- If the weekly hours rise by 1 base point - meaning rise by 10 hours - this will result to Â£1032.82 rise in the annual average salary.\n",
    "- If the Job title rise by 1 base point - meaning rise by one hierarchy level (from Apprentice to Assistant)- this will result to Â£4633.97 rise in the annual average salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffe529",
   "metadata": {},
   "source": [
    "Note: Due to the fact that we form our hypothesis that late in the process and not at the beginning the choice of the predictors and the results will contain some Bias. Probably we can avoid this Bias problem if we formulate our hypothesis in the beginning, even before web scrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173494d",
   "metadata": {},
   "source": [
    "Checking about Correlation between our predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0b852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.397834Z",
     "start_time": "2021-11-03T20:13:00.384878Z"
    }
   },
   "outputs": [],
   "source": [
    "correlation_matrix = data.corr(method='pearson', min_periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a76306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T20:13:00.429835Z",
     "start_time": "2021-11-03T20:13:00.400838Z"
    }
   },
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6ffd9",
   "metadata": {},
   "source": [
    "Result: Based on the correlation matrix we can see that 'Job_title_Num' predictor is highly correlated with the 'Weekly hours' and this can create a problem in both regression because we can't separate properly the effect of this to predictors in the response. So probably this is not the most appropriate regression model for us to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
